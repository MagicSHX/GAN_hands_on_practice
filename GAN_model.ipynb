{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mnist import MNIST\n",
    "import torch\n",
    "from torch import nn\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data_file_path = \"Data/fake_data_generated.csv\"\n",
    "Discriminator_model_file_path = \"Model/Discriminator_model.pt\"\n",
    "GAN_model_file_path = \"Model/GAN_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_array_generation(array_size, no_of_array):\n",
    "    array_generated = np.random.rand(no_of_array, array_size)\n",
    "    return array_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mndata = MNIST('Data')\n",
    "#images, labels = mndata.load_training()\n",
    "images, labels = mndata.load_testing()\n",
    "\n",
    "#print(labels)\n",
    "len(images[0])\n",
    "\n",
    "real_input_tensor = torch.Tensor(images)\n",
    "real_output_tensor = torch.Tensor([1 for i in range(real_input_tensor.size(0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = 1\n",
    "if fake_data == 0:\n",
    "    array_generated = random_array_generation(100, 10000)\n",
    "    np.savetxt(fake_data_file_path, array_generated, delimiter=\",\")\n",
    "else:\n",
    "    array_generated = pd.read_csv(fake_data_file_path, header = None)\n",
    "\n",
    "fake_input_tensor = torch.Tensor(array_generated.values)\n",
    "fake_output_tensor = torch.Tensor([0 for i in range(fake_input_tensor.size(0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8113, 0.8830, 0.1161, 0.9041, 0.8295, 0.5640, 0.9597, 0.6732, 0.0773,\n",
       "        0.1733, 0.2507, 0.6257, 0.9794, 0.8131, 0.3409, 0.2037, 0.1561, 0.0355,\n",
       "        0.5426, 0.2767, 0.3555, 0.7762, 0.0477, 0.5486, 0.8409, 0.4584, 0.2002,\n",
       "        0.7612, 0.9665, 0.6159, 0.1208, 0.2811, 0.6921, 0.0898, 0.7824, 0.8461,\n",
       "        0.4351, 0.9992, 0.0376, 0.0379, 0.6627, 0.2711, 0.9265, 0.7212, 0.3231,\n",
       "        0.1601, 0.4817, 0.1492, 0.5890, 0.4626, 0.2383, 0.6736, 0.7089, 0.3591,\n",
       "        0.5928, 0.4461, 0.3668, 0.2426, 0.3939, 0.3521, 0.2196, 0.7634, 0.2150,\n",
       "        0.8353, 0.6717, 0.5992, 0.1051, 0.9078, 0.4213, 0.4814, 0.1232, 0.9127,\n",
       "        0.4303, 0.7778, 0.7342, 0.6365, 0.2763, 0.2299, 0.8172, 0.6221, 0.7781,\n",
       "        0.4370, 0.7914, 0.7638, 0.9722, 0.0175, 0.2349, 0.3645, 0.2698, 0.4705,\n",
       "        0.3708, 0.1050, 0.1174, 0.5793, 0.8773, 0.4560, 0.3683, 0.7286, 0.3473,\n",
       "        0.1747])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_input_tensor[0]\n",
    "#real_output_tensor\n",
    "fake_input_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_input_tensor.size(0)\n",
    "fake_output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN_Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(GAN_Model, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 512)\n",
    "        self.fc3 = nn.Linear(512, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator_Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Discriminator_Model, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8113, 0.8830, 0.1161, 0.9041, 0.8295, 0.5640, 0.9597, 0.6732, 0.0773,\n",
      "        0.1733, 0.2507, 0.6257, 0.9794, 0.8131, 0.3409, 0.2037, 0.1561, 0.0355,\n",
      "        0.5426, 0.2767, 0.3555, 0.7762, 0.0477, 0.5486, 0.8409, 0.4584, 0.2002,\n",
      "        0.7612, 0.9665, 0.6159, 0.1208, 0.2811, 0.6921, 0.0898, 0.7824, 0.8461,\n",
      "        0.4351, 0.9992, 0.0376, 0.0379, 0.6627, 0.2711, 0.9265, 0.7212, 0.3231,\n",
      "        0.1601, 0.4817, 0.1492, 0.5890, 0.4626, 0.2383, 0.6736, 0.7089, 0.3591,\n",
      "        0.5928, 0.4461, 0.3668, 0.2426, 0.3939, 0.3521, 0.2196, 0.7634, 0.2150,\n",
      "        0.8353, 0.6717, 0.5992, 0.1051, 0.9078, 0.4213, 0.4814, 0.1232, 0.9127,\n",
      "        0.4303, 0.7778, 0.7342, 0.6365, 0.2763, 0.2299, 0.8172, 0.6221, 0.7781,\n",
      "        0.4370, 0.7914, 0.7638, 0.9722, 0.0175, 0.2349, 0.3645, 0.2698, 0.4705,\n",
      "        0.3708, 0.1050, 0.1174, 0.5793, 0.8773, 0.4560, 0.3683, 0.7286, 0.3473,\n",
      "        0.1747])\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "learning_rate_Discriminator = 0.08\n",
    "epoch_size_Discriminator = 5\n",
    "steps_for_printing_out_loss_Discriminator = 5\n",
    "\n",
    "Discriminator_Model_WIP = Discriminator_Model(input_size = 784, output_size = 1)\n",
    "\n",
    "#RNN_model.to(device)\n",
    "loss_functioin_Discriminator = nn.MSELoss()\n",
    "optimizer_Discriminator = optim.SGD(Discriminator_Model_WIP.parameters(), lr = learning_rate_Discriminator)\n",
    "\n",
    "input_real = real_input_tensor/ 256\n",
    "target_real = real_output_tensor\n",
    "#input_fake = fake_input_tensor\n",
    "target_fake = fake_output_tensor\n",
    "print(fake_input_tensor[0])\n",
    "def Discriminator_training_model():\n",
    "    input_fake = GAN_Model_WIP(fake_input_tensor)\n",
    "    #print(input_fake[0])\n",
    "    for i in range(1, epoch_size_Discriminator + 1):\n",
    "        optimizer_Discriminator.zero_grad()\n",
    "        output_real = Discriminator_Model_WIP(input_real)\n",
    "        loss_real = loss_functioin_Discriminator(output_real, target_real.reshape(output_real.size(0), output_real.size(1)))\n",
    "        loss_real.backward()\n",
    "        #loss_real.backward()\n",
    "        output_fake = Discriminator_Model_WIP(input_fake)\n",
    "        #print(output_real)\n",
    "        loss_fake = loss_functioin_Discriminator(output_fake, target_fake.reshape(output_fake.size(0), output_fake.size(1)))\n",
    "        loss_fake.backward(retain_graph=True)\n",
    "        #loss_fake.backward()\n",
    "        optimizer_Discriminator.step()\n",
    "        loss_Discriminator = loss_real + loss_fake\n",
    "        print(loss_real.cpu().detach().numpy())\n",
    "        print(loss_fake.cpu().detach().numpy())\n",
    "        \n",
    "        if i % (steps_for_printing_out_loss_Discriminator) == 0:\n",
    "            print('Loss (epoch: ' + str(i) + '): ' + str(loss_Discriminator.cpu().detach().numpy()))\n",
    "    torch.save({'state_dict': Discriminator_Model_WIP.state_dict(),'optimizer': optimizer_Discriminator.state_dict()}, Discriminator_model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "learning_rate_GAN = 0.28\n",
    "epoch_size_GAN = 5\n",
    "steps_for_printing_out_loss_GAN = 1\n",
    "\n",
    "GAN_Model_WIP = GAN_Model(input_size = 100, output_size = 784)\n",
    "\n",
    "#RNN_model.to(device)\n",
    "loss_functioin_GAN = nn.MSELoss()\n",
    "optimizer_GAN = optim.SGD(GAN_Model_WIP.parameters(), lr = learning_rate_GAN)\n",
    "input_GAN = fake_input_tensor\n",
    "target_GAN = torch.Tensor([1 for i in range(input_GAN.size(0))])\n",
    "def GAN_training_model():\n",
    "    for i in range(1, epoch_size_GAN + 1):\n",
    "        optimizer_GAN.zero_grad()\n",
    "        output_GAN = Discriminator_Model_WIP(GAN_Model_WIP(input_GAN))\n",
    "        loss_GAN = loss_functioin_GAN(output_GAN, target_GAN.reshape(output_GAN.size(0), output_GAN.size(1)))\n",
    "        loss_GAN.backward()\n",
    "        if i % (steps_for_printing_out_loss_GAN) == 0:\n",
    "            print('Loss (epoch: ' + str(i) + '): ' + str(loss_GAN.cpu().detach().numpy()))\n",
    "        optimizer_GAN.step()\n",
    "\n",
    "    torch.save({'state_dict': GAN_Model_WIP.state_dict(),'optimizer': optimizer_GAN.state_dict()}, GAN_model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.021590779\n",
      "0.7001362\n",
      "Loss (epoch: 1): 0.72172695\n",
      "0.33205017\n",
      "0.0009096491\n",
      "Loss (epoch: 2): 0.33295983\n",
      "0.08519983\n",
      "0.059007097\n",
      "Loss (epoch: 3): 0.14420693\n",
      "0.05604492\n",
      "0.00411944\n",
      "Loss (epoch: 4): 0.060164362\n",
      "0.026023077\n",
      "0.0028505072\n",
      "Loss (epoch: 5): 0.028873583\n",
      "Loss (epoch: 1): 0.9725793\n",
      "Loss (epoch: 2): 0.6419667\n",
      "Loss (epoch: 3): 0.32240754\n",
      "Loss (epoch: 4): 0.15377317\n",
      "Loss (epoch: 5): 0.075545155\n",
      "0.02199151\n",
      "0.6657448\n",
      "Loss (epoch: 1): 0.6877363\n",
      "0.36001343\n",
      "0.0\n",
      "Loss (epoch: 2): 0.36001343\n",
      "0.08116432\n",
      "0.028412288\n",
      "Loss (epoch: 3): 0.10957661\n",
      "0.046413682\n",
      "0.0015615886\n",
      "Loss (epoch: 4): 0.047975272\n",
      "0.02943387\n",
      "0.001696395\n",
      "Loss (epoch: 5): 0.031130265\n",
      "Loss (epoch: 1): 0.9811701\n",
      "Loss (epoch: 2): 0.74059707\n",
      "Loss (epoch: 3): 0.41492695\n",
      "Loss (epoch: 4): 0.20665906\n",
      "Loss (epoch: 5): 0.10190102\n",
      "0.026598334\n",
      "0.61246765\n",
      "Loss (epoch: 1): 0.639066\n",
      "0.2900401\n",
      "0.011055185\n",
      "Loss (epoch: 2): 0.30109528\n",
      "0.09848419\n",
      "0.031080693\n",
      "Loss (epoch: 3): 0.12956488\n",
      "0.051352322\n",
      "0.002062765\n",
      "Loss (epoch: 4): 0.053415086\n",
      "0.026647128\n",
      "0.003191461\n",
      "Loss (epoch: 5): 0.02983859\n",
      "Loss (epoch: 1): 0.9825552\n",
      "Loss (epoch: 2): 0.75131476\n",
      "Loss (epoch: 3): 0.3905987\n",
      "Loss (epoch: 4): 0.16550052\n",
      "Loss (epoch: 5): 0.061884012\n",
      "0.02430236\n",
      "0.71688086\n",
      "Loss (epoch: 1): 0.7411832\n",
      "0.3366621\n",
      "0.0076607517\n",
      "Loss (epoch: 2): 0.34432286\n",
      "0.10512548\n",
      "0.041152548\n",
      "Loss (epoch: 3): 0.14627802\n",
      "0.05407042\n",
      "0.0055399174\n",
      "Loss (epoch: 4): 0.059610337\n",
      "0.027730515\n",
      "0.0028879372\n",
      "Loss (epoch: 5): 0.030618452\n",
      "Loss (epoch: 1): 0.96980584\n",
      "Loss (epoch: 2): 0.54801387\n",
      "Loss (epoch: 3): 0.12742339\n",
      "Loss (epoch: 4): 0.042680323\n",
      "Loss (epoch: 5): 0.009293599\n",
      "0.023843085\n",
      "0.9423787\n",
      "Loss (epoch: 1): 0.9662218\n",
      "0.47779405\n",
      "0.008571099\n",
      "Loss (epoch: 2): 0.48636514\n",
      "0.20322259\n",
      "0.07291671\n",
      "Loss (epoch: 3): 0.2761393\n",
      "0.12797605\n",
      "0.026025783\n",
      "Loss (epoch: 4): 0.15400183\n",
      "0.063608296\n",
      "0.013128474\n",
      "Loss (epoch: 5): 0.07673677\n",
      "Loss (epoch: 1): 0.87107694\n",
      "Loss (epoch: 2): 0.38003442\n",
      "Loss (epoch: 3): 0.15481873\n",
      "Loss (epoch: 4): 0.07492864\n",
      "Loss (epoch: 5): 0.035097264\n",
      "0.033129398\n",
      "0.7773776\n",
      "Loss (epoch: 1): 0.810507\n",
      "0.33965397\n",
      "0.03959309\n",
      "Loss (epoch: 2): 0.37924707\n",
      "0.17335571\n",
      "0.057129767\n",
      "Loss (epoch: 3): 0.23048548\n",
      "0.10614828\n",
      "0.025635665\n",
      "Loss (epoch: 4): 0.13178395\n",
      "0.05546809\n",
      "0.010062926\n",
      "Loss (epoch: 5): 0.065531015\n",
      "Loss (epoch: 1): 0.86561805\n",
      "Loss (epoch: 2): 0.3546895\n",
      "Loss (epoch: 3): 0.1604795\n",
      "Loss (epoch: 4): 0.07834732\n",
      "Loss (epoch: 5): 0.04092878\n",
      "0.028021999\n",
      "0.73023015\n",
      "Loss (epoch: 1): 0.75825214\n",
      "0.25047335\n",
      "0.051885366\n",
      "Loss (epoch: 2): 0.30235872\n",
      "0.13076101\n",
      "0.031998932\n",
      "Loss (epoch: 3): 0.16275994\n",
      "0.072795264\n",
      "0.009323391\n",
      "Loss (epoch: 4): 0.08211865\n",
      "0.03858548\n",
      "0.0060356543\n",
      "Loss (epoch: 5): 0.044621136\n",
      "Loss (epoch: 1): 0.9439578\n",
      "Loss (epoch: 2): 0.54284376\n",
      "Loss (epoch: 3): 0.3318142\n",
      "Loss (epoch: 4): 0.2012841\n",
      "Loss (epoch: 5): 0.10924684\n",
      "0.030566962\n",
      "0.5877061\n",
      "Loss (epoch: 1): 0.6182731\n",
      "0.25272614\n",
      "0.013514453\n",
      "Loss (epoch: 2): 0.2662406\n",
      "0.08331261\n",
      "0.03699412\n",
      "Loss (epoch: 3): 0.12030673\n",
      "0.06234287\n",
      "0.0003900385\n",
      "Loss (epoch: 4): 0.06273291\n",
      "0.034170035\n",
      "0.005681092\n",
      "Loss (epoch: 5): 0.03985113\n",
      "Loss (epoch: 1): 0.9940534\n",
      "Loss (epoch: 2): 0.88368857\n",
      "Loss (epoch: 3): 0.41550732\n",
      "Loss (epoch: 4): 0.195591\n",
      "Loss (epoch: 5): 0.103204876\n",
      "0.032183826\n",
      "0.60347104\n",
      "Loss (epoch: 1): 0.63565487\n",
      "0.25885576\n",
      "2.3452561e-05\n",
      "Loss (epoch: 2): 0.2588792\n",
      "0.0744848\n",
      "0.062398463\n",
      "Loss (epoch: 3): 0.13688326\n",
      "0.08905093\n",
      "4.2784457e-05\n",
      "Loss (epoch: 4): 0.089093715\n",
      "0.05384678\n",
      "0.008729661\n",
      "Loss (epoch: 5): 0.06257644\n",
      "Loss (epoch: 1): 0.98655796\n",
      "Loss (epoch: 2): 0.7863312\n",
      "Loss (epoch: 3): 0.42527142\n",
      "Loss (epoch: 4): 0.24005392\n",
      "Loss (epoch: 5): 0.12446949\n",
      "0.0524407\n",
      "0.60390675\n",
      "Loss (epoch: 1): 0.65634745\n",
      "0.31321502\n",
      "9.8368495e-05\n",
      "Loss (epoch: 2): 0.3133134\n",
      "0.099105425\n",
      "0.048177067\n",
      "Loss (epoch: 3): 0.1472825\n",
      "0.08294412\n",
      "0.0037366124\n",
      "Loss (epoch: 4): 0.08668073\n",
      "0.06138086\n",
      "0.0039098416\n",
      "Loss (epoch: 5): 0.065290704\n",
      "Loss (epoch: 1): 0.9563785\n",
      "Loss (epoch: 2): 0.6067841\n",
      "Loss (epoch: 3): 0.4165868\n",
      "Loss (epoch: 4): 0.30779958\n",
      "Loss (epoch: 5): 0.20717052\n"
     ]
    }
   ],
   "source": [
    "no_of_training_loop = 10\n",
    "for i in range(no_of_training_loop):\n",
    "    Discriminator_training_model()\n",
    "    GAN_training_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fake = GAN_Model_WIP(fake_input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_out = (input_real[0]).reshape(28, 28).cpu().detach().numpy()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "#img=mpimg.imread('your_image.png')\n",
    "imgplot = plt.imshow(image_out)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0793, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.6248, 0.0000, 0.0098, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0757, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0198,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0045,\n",
       "        0.0000, 0.0364, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0859, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0077,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3110, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.3814, 0.0068, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0005, 0.2195,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3234,\n",
       "        0.0000, 0.3195, 0.0000, 0.0000, 0.0000, 0.5443, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.3182, 0.0000, 0.0000, 0.0000, 0.0000, 0.0275, 0.0000,\n",
       "        0.4494, 0.0823, 0.1443, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4383,\n",
       "        0.0000, 0.0000, 0.0000, 0.0008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2926, 0.3464, 0.5021,\n",
       "        0.3729, 0.0000, 0.0000, 0.5721, 0.3740, 0.4335, 0.0000, 0.2394, 0.0000,\n",
       "        0.0042, 0.3093, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.1726, 0.0000, 0.1200, 0.0000, 0.0000, 0.0000,\n",
       "        0.0102, 1.0727, 0.5152, 0.0804, 0.0000, 0.3716, 0.5683, 0.0000, 0.3216,\n",
       "        0.5868, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1251, 0.0000, 0.1563,\n",
       "        0.0000, 0.0135, 0.4155, 0.1677, 0.6897, 0.0306, 1.0180, 0.4725, 0.0000,\n",
       "        0.0000, 0.6645, 0.5062, 0.0000, 0.0227, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.1555, 0.2049, 0.0043, 0.0000, 0.0000, 0.6781,\n",
       "        0.0000, 0.1884, 0.0000, 0.0000, 0.0000, 0.2107, 0.0000, 0.2350, 0.3347,\n",
       "        0.4850, 0.4658, 0.0000, 0.6348, 0.0000, 0.8232, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0787, 0.0000, 0.0000, 0.0000, 0.3680,\n",
       "        0.0000, 0.0638, 0.0000, 0.7638, 0.3900, 0.0000, 0.4847, 0.3880, 0.3920,\n",
       "        0.0000, 0.0000, 0.1249, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0121, 0.0000, 0.2068,\n",
       "        0.1331, 0.0000, 0.0000, 0.0000, 0.0463, 1.4059, 0.4494, 0.0419, 2.0225,\n",
       "        0.1455, 0.7443, 0.0000, 0.0000, 0.0000, 0.1165, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.8059, 0.0000, 0.2810, 0.0000, 0.2660,\n",
       "        0.6243, 1.0526, 0.5732, 0.0000, 0.4901, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2265, 0.0000, 0.0000,\n",
       "        0.0000, 0.9215, 0.0119, 0.0000, 0.0000, 0.7897, 0.7270, 0.3215, 0.0959,\n",
       "        0.1156, 0.0000, 0.3144, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6053,\n",
       "        0.0000, 0.1284, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1913, 0.0000,\n",
       "        1.2098, 1.0288, 0.7614, 0.0000, 0.0000, 0.5439, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0281, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.1981, 0.0000, 0.0000, 0.1497, 0.0000, 0.1819, 0.3650, 0.2540, 0.0000,\n",
       "        0.0000, 0.5447, 1.0288, 0.6716, 0.6962, 0.0000, 0.0000, 0.0695, 0.0621,\n",
       "        0.0000, 0.0000, 0.0000, 0.0201, 0.0000, 0.7513, 0.0942, 0.0000, 0.0000,\n",
       "        0.0798, 0.0000, 0.0000, 0.0000, 0.2839, 0.2867, 0.0000, 0.0000, 0.0600,\n",
       "        1.0718, 0.2474, 0.6866, 0.4239, 0.0787, 0.0000, 0.8687, 0.0000, 0.0681,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3302, 0.0000, 0.5801,\n",
       "        0.0302, 0.0000, 0.4949, 0.2077, 0.0000, 0.0000, 0.5737, 0.3585, 0.0000,\n",
       "        0.7405, 0.0000, 0.3111, 0.0122, 0.0000, 0.0988, 0.0000, 0.2932, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1371, 0.3759,\n",
       "        0.5468, 0.4210, 0.6611, 0.1889, 0.3622, 0.7448, 0.4476, 0.0000, 0.0000,\n",
       "        0.2161, 0.0000, 0.0000, 0.0000, 0.0000, 0.5774, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.2280, 0.0436, 1.0881, 0.0000, 0.5027, 0.3677, 0.5741, 0.0000,\n",
       "        0.0069, 0.2693, 0.0000, 0.6113, 0.0000, 0.2816, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.1108, 0.0000, 0.0000, 0.1280, 0.0000, 0.0000, 0.2526,\n",
       "        0.1115, 0.0000, 0.4505, 0.7577, 0.3978, 0.0000, 0.5776, 0.1852, 0.0000,\n",
       "        0.0000, 0.4252, 0.7153, 0.0000, 0.0000, 0.3700, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1587, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8517, 0.0034, 0.0000,\n",
       "        0.2496, 0.7340, 0.1105, 0.0000, 0.2513, 0.0000, 0.0013, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0178, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.8035, 0.1348, 0.0000, 0.2925, 0.4450, 0.8149, 0.0000,\n",
       "        0.0000, 0.0000, 0.0286, 0.7844, 0.0000, 0.0000, 0.0000, 0.0098, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.1886, 0.0000, 0.0000, 0.0000, 0.0000, 0.5108, 0.0000,\n",
       "        0.4977, 0.0000, 0.1884, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3748, 0.0000, 0.0000,\n",
       "        0.0000, 0.2358, 0.6970, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2732, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.3186, 0.0000, 0.0000, 0.3488, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0290, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0070, 0.0000, 0.0000, 0.1721, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0471, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0138, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(input_fake[1] ==input_fake[220])\n",
    "input_fake[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
